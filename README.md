# Data harvesting, ASR, and packaging tools

This repository is a collection of tools and scripts developed during the Data Harvesting Phase 3 project, and contains:

- **Data harvesting & data pre‑processing** scripts for speech corpora.
- **Automatic speech recognition (ASR)** recipes adapted from the [Icefall repository](https://github.com/k2-fsa/icefall).
- **Packaging** tools for converting cleaned/annotated data into reusable corpus formats.

This repository contains the full pipeline from raw audio + transcripts → ASR models → packaged corpora.

---

## Repository layout (top level)

From the dh_gitlab root:

- `data/`   
  Data harvesting and text/audio processing scripts.
  
- `ASR/`    
  ASR experiments and tools: feature extraction for ASR, model training/decoding, inference, and scoring.
  
- `packaging/`    
  Tools to package dataset into an XML corpora with defined schemas and APIs.

---

## data/ – data preparation & text processing

This directory collects tools for cleaning, normalising, and organising raw data before it enters ASR pipelines or corpora.

Typical contents include:

- `convert_mp3_to_wav.py`   
  Convert source audio (recorded in `.mp3` format) to a consistent WAV format.

- `change_filenaming_convention_text.py`    
  Adjust or standardise filenaming conventions for text/audio so downstream tools can parse IDs reliably.

- `convert_spreadsheet_transcriptions_to_textfiles.py`    
  Saves the content in transcript spreadsheets verified by external transcription companies into plain text files with one file per segment.

- `text_cleaning_and_normalization.py` and `data/textnorm/`    
  Text‑cleaning and normalisation utilities (e.g. punctuation, casing, tokenisation rules for specific languages/projects).

- `data/segmentation/`   
  Uses silence detection in Kaldi 1 to segment a long piece of audio into multiple shorter segments.

- `data/processed_outputs/`    
  Each SAMA news corpus contains at least 10 hours of manually verified transcriptions that were organized into validation (~30 min), test (~30 min) and training sets (all remaining segments). This directory contains a compilation of training-set transcriptions from each SAMA corpus. This file can be used as input data to augment language models and excludes all transcripts from the suggested SAMA validation and test set transcriptions.

---

## ASR/ – ASR tools and experiments

This subdirectory contains ASR‑specific scripts and recipes, built on top of libraries such as Lhotse, K2/icefall, PyTorch, Vosk, etc.

High‑level structure:

- `ASR/models/`   
  ASR model recipes, data preparation, training, and decoding:
  - `ASR/models/WSASR/` – Weakly supervised ASR recipe from Icefall
  - `ASR/models/zipformer/` – Zipformer‑based ASR recipe from Icefall
  - `ASR/models/conformer_ctc2/` – Conformer CTC-based ASR recipe from Icefall
  - `ASR/models/RNN_LM/` – Scripts to train an RNN-based language model
  - `ASR/models/bash_scripts/` – Bash scripts to run the language modelling and ASR trainin pipelines

- `ASR/inference/`    
  Lightweight inference tools:
  - Run ASR models on user-defined lists of filenames.
  - Use Vosk for simpler decoding pipelines.

- `ASR/DP_scoring/`   
  Dynamic‑programming based scoring tools for `.tra` label files (e.g. comparing hypothesis vs. reference label sequences).

---

## packaging/ – corpus packaging and APIs

This directory is for turning processed data into a structured corpus that can be shared.

Typical components:

- `build_corpus.py`   
  Scripts & utilities to build a corpus from processed data (e.g. assembling metadata, transcripts, and audio into a coherent XML structure).

- `corpus_api.py`   
  An API layer generated by GenerateDS for interacting with the packed corpus (querying entries, metadata, etc.).

- `corpus_xml_structure.xsd`    
  XML schema definition for the corpus format (e.g. for radio news data).

- `corpus_config.yaml`    
  Example configuration file containing dataset-specific metadata (e.g. corpus name, source, genre and audio_format)
