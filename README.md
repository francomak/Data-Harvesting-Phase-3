# Data harvesting, ASR, and packaging tools

This repository is a collection of tools and scripts used for:

- **Data harvesting & data pre‑processing** for speech corpora.
- **Automatic speech recognition (ASR)** experiments.
- **Packaging** cleaned/annotated data into reusable corpus formats.

This repository contains  the full pipeline from raw audio + transcripts → ASR models → packaged corpora.

---

## Repository layout (top level)

From the dh_gitlab root:

- data/
  Data harvesting and text/audio processing scripts.
- ASR/
  ASR experiments and tools: feature extraction for ASR, model training/decoding, inference, and scoring.
- packaging/
  Tools to package dataset into an XML corpora with defined schemas and APIs.

---

## data/ – data preparation & text processing

This directory collects tools for cleaning, normalising, and organising raw data before it enters ASR pipelines or corpora.

Typical contents include:

- convert_mp3_to_wav.py
  Convert source audio (recorded in `.mp3` format) to a consistent WAV format.

- change_filenaming_convention_text.py
  Adjust or standardise filenaming conventions for text/audio so downstream tools can parse IDs reliably.

- convert_spreadsheet_transcriptions_to_textfiles.py
  Save vendor‑supplied spreadsheets into plain text files with one file per segment.

- text_cleaning_and_normalization.py and textnorm/
  Text‑cleaning and normalisation utilities (e.g. punctuation, casing, tokenisation rules for specific languages/projects).

- segmentation/
  Uses silence detection in Kaldi 1 to segment a long piece of audio into multiple shorter segments

- processed_outputs/
  Contains a compilation of training-set transcriptions for news datasets from each manually verified SAMA news corpora. This compilation file can be used as input data to augment language models without leaking our suggested validation and test sets transcriptions

---

## ASR/ – ASR tools and experiments

This subdirectory contains ASR‑specific scripts and recipes, built on top of libraries such as Lhotse, K2/icefall, PyTorch, Vosk, etc.

High‑level structure:

- models/
  ASR model recipes, data preparation, training, and decoding:
  - models/WSASR/ – weakly supervised ASR recipe from Icefall
  - models/zipformer/ – Zipformer‑based ASR recipe from Icefall
  - models/conformer_ctc2/ – Conformer CTC-based ASR recipe from Icefall
  - models/RNN_LM/ – scripts to train an RNN-based language model
  - models/bash_scripts/ – bash scripts to run the language modelling and ASR trainin pipelines

- inference/
  Lightweight inference tools:
  - Run ASR models on user-defined lists of filenames.
  - Use Vosk for simpler decoding pipelines.

- DP_scoring/
  Dynamic‑programming based scoring tools for `.tra` label files (e.g. comparing hypothesis vs. reference label sequences).

---

## packaging/ – corpus packaging and APIs

This directory is for turning processed data into a structured corpus that can be shared.

Typical components:

- build_corpus.py
  Scripts/utilities to build a corpus from processed data (e.g. assembling metadata, transcripts, and audio into a coherent XML structure).

- corpus_api.py
  An API layer generated by GenerateDS for interacting with the packed corpus (querying entries, metadata, etc.).

- corpus_xml_structure.xsd 
  XML schema definition for the corpus format (e.g. for radio news data).

- corpus_config.yaml
  Example configuration file containing dataset-specific metadata (e.g. corpus name, source, genre and audio_format)
