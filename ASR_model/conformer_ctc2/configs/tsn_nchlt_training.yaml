---
dataset:    # Description of training dataset. Mainly used in Lhotse_data_preparation.py
    lang: tsn
    audio_dir: path/to/audio_dir
    transcription_dir: path/to/transcription_dir
    lists_dir: path/to/lists_dir    ## This dir contains textfiles for train/dev/test splits. Each textfile contains a list of filenames that are assigned to each split. The filenames correspond to those found in audio_dir and transcription_dir
    train_list: nchlt_tsn.trn.txt
    dev_list: nchlt_tsn.dev.lst
    test_list: nchlt_tsn.tst.lst
    gender_in_filename: yes          ## Choose between "yes" or "no". NCHLT speaker folders contain gender in the name, for example 'm' for male in "nchlt_afr_001m_0003.wav", but broadcast news filenames do not

datamodule:
    manifest_dir: "monolingual/tsn/data"    # Path to directory with train/dev/test cuts
    lang_dir: "monolingual/tsn/data/lang_phone"     # The lang dir. It contains language related input files such as lexicon.txt
    lm_dir: "monolingual/tsn/data/lm"    # The n-gram LM dir. It should contain either G_4_gram.pt or G_4_gram.fst.txt

    train_cuts: "tsn_nchlt_train_cuts.jsonl" # Lhotse cutset containing training segments
    dev_cuts: "tsn_nchlt_dev_cuts.jsonl"     # Lhotse cutset containing dev segments
    test_cuts: "tsn_nchlt_test_cuts.jsonl"   # Lhotse cutset containing test segments

train:
    exp_dir: "languages/tsn/exp_nchlt"   # experiment directory to store epoch checkpoints
    start_epoch: 1              # Resume training from this epoch. It should be positive. If larger than 1, it will load checkpoint from exp-dir/epoch-{start_epoch-1}.pt
    num_epochs: 20              # Number of epochs to train.
    num_decoder_layers: 0       # Number of decoder layer of transformer decoder. Setting this to 0 will not create the decoder at all (pure CTC model)

parameters:
    attention_rate: 0           # The total loss is (1 -  att_rate) * ctc_loss + att_rate * att_loss
    num_buckets: 30             # The number of buckets for the DynamicBucketingSampler (you might want to increase it for larger datasets)
    num_workers: 4              # The number of training dataloader workers that collect the batches

    # Parameters below are usually left on their default values:
    initial_lr: 0.003           # The initial learning rate. This value should not need to be changed
    lr_epochs: 6                # Number of epochs that affects how rapidly the learning rate decreases
    max_duration: 170           # Maximum pooled recordings duration (seconds) in a single batch. You can reduce it if it causes CUDA OOM
    use_fp16: False             # Whether to use half precision training

decode:     # Parameters used in decode.py
    average: 1                  # Number of checkpoints to average. Automatically select consecutive checkpoints before the checkpoint specified by --epoch
    method: "1best"             # Available options are: ctc-decoding, ctc-greedy-search, 1best, nbest, nbest-rescoring, whole-lattice-rescoring, attention-decoder, rnn-lm, and nbest-oracle
    use_averaged_model: False   # Whether to load averaged model. If True, it would decode with the averaged model over the epoch range from `epoch-avg` (excluded) to `epoch`
    num_decoder_layers: 0       # Number of decoder layers of transformer decoder. Setting this to 0 will not create the decoder at all (pure CTC model)
    output_filename: "cmd_outputs.txt" #  Textfile to store terminal printouts during decode

    # RNN language model related parameters
    rnn_lm_dir: 'rnn_lm/exp'    # Used only when --method is rnn-lm. It specifies the path to RNN LM exp dir
    rnn_lm_epoch: 10            # Used only when --method is rnn-lm. It specifies the checkpoint to use.
    rnn_lm_avg: 2               # Used only when --method is rnn-lm. It specifies the number of checkpoints to average.
    rnn_lm_num_layers: 4        # Number of RNN layers in the model
    rnn_lm_embedding_dim: 2048  # Embedding dim of the model
    rnn_lm_hidden_dim: 2048     # Hidden dim of the model
    rnn_lm_tie_weights: False   # True to share the weights between the input embedding layer and the last output linear layer

    # Other parameters:
    num_paths: 100              # Number of paths for n-best based decoding method. Used only when "method" is: nbest, nbest-rescoring, attention-decoder, rnn-lm, and nbest-oracle
    nbest_scale: 0.5            # The scale to be applied to `lattice.scores`. It's needed if you use any kinds of n-best based rescoring. Used only when "method" is one of the following values: nbest, nbest-rescoring, attention-decoder, rnn-lm, and nbest-oracle. A smaller value results in more unique paths.
...
